Training and Checkpointing
==========================

Distributed Training Configuration
----------------------------------

To configure distributed training, run:

.. code-block:: bash

    $ accelerate config

We are currently using DeepSpeed for model parallelism distributed training. For an accelerate config example, see `this config file <../../examples/accelerate_ci_config.yaml>`_ used for our CI pipelines.

To train across multiple GPUs, run the following command:

.. code-block:: bash

    $ accelerate launch lerobot/scripts/train.py --config_path=examples/dev_config.json

This uses the default accelerate config file set by running ``accelerate config``.

To use a specific accelerate config, run:

.. code-block:: bash

    $ accelerate launch --config_file=examples/accelerate_ci_config.yaml lerobot/scripts/train.py --config_path=examples/dev_config.json


Checkpointing and Resuming Training
-----------------------------------

Start training and saving checkpoints:

.. code-block:: bash

    $ accelerate launch lerobot/scripts/train.py --config_path=examples/dev_config.json --output_dir=outputs/train/tau0 --steps 40 --log_freq 5 --save_freq 10

A checkpoint should be saved at step 40. The checkpoint should be saved in the directory ``outputs/train/tau0/checkpoints/000040/``.

The ``model.safetensors`` file is not automatically generated by DeepSpeed's checkpointing during training. To consolidate the sharded model checkpoint files generated by DeepSpeed into a single ``model.safetensors`` file, run:

.. code-block:: bash

    $ ./convert_checkpoint.sh outputs/train/tau0/checkpoints/000040/

This generates a ``model.safetensors`` file that can be used for inference or resuming training.

Training can be resumed by running:

.. code-block:: bash

    $ accelerate launch lerobot/scripts/train.py --config_path=outputs/train/tau0/checkpoints/000040/train_config.json --resume=true --steps=100

It should be noted that the training step count continues from the checkpoint step count, but the dataloader is reset.

Optional TODO: Restore the state of the dataloader:
https://huggingface.co/docs/accelerate/v1.6.0/en/usage_guides/checkpoint#restoring-the-state-of-the-dataloader

