# AutoX Humanoid VLA


[![pipeline status](https://code.autox.ds/xerobot/lerobot/badges/main/pipeline.svg)](https://code.autox.ds/xerobot/lerobot/-/commits/main)
![Coverage](https://code.autox.ds/xerobot/lerobot/badges/main/coverage.svg?job=combine_coverage)


## AutoX Installation
Download the source code:
```bash
$ git clone git@code.autox.ds:xisp/agi/lerobot.git
$ cd lerobot
```

We use [uv](https://docs.astral.sh/uv/) to manage Python dependencies as it is easier and faster to use than Conda. If you would still like to use Conda, see the Conda environment setup below in the LeRobot installation instructions. See the [uv installation instructions](https://docs.astral.sh/uv/getting-started/installation/) to set it up. Once uv is installed, run the following to set up the environment:

If you haven't already, please install cmake 3.x using `apt`, `brew`, or another package manager.

```bash
$ uv sync --extra tau0 --extra test --extra video_benchmark --extra accelerate --extra dev --extra feetech --extra openai --extra onnx --extra smolvla --extra libero
$ source .venv/bin/activate
```
Note that PI0.5 and Tau0/PI0 are not compatible with each other due to different `transformers` package versions. If you need to run PI0.5, run the following command which will override the `transformers` package version:
```bash
$ uv pip install -r requirements-pi05.txt
```


To use [Weights and Biases](https://docs.wandb.ai/quickstart) for experiment tracking, log in with
```bash
$ wandb login
```

To configure distributed training, run:
```bash
$ accelerate config
```
We are currently using DeepSpeed for model parallelism distributed training. For an accelerate config example, see [this config file](examples/accelerate_ci_config.yaml) used for our CI pipelines.

To train across multiple GPUs, run the following command:
```bash
$ accelerate launch lerobot/scripts/train.py --config_path=examples/dev_config.json
```
This uses the default accelerate config file set by running `accelerate config`.

To use a specific accelerate config, run:
```bash
$ accelerate launch --config_file=examples/accelerate_ci_config.yaml lerobot/scripts/train.py --config_path=examples/dev_config.json
```

## Checkpointing and Resuming Training Tutorial

Start training and saving checkpoints:
```bash
$ accelerate launch lerobot/scripts/train.py --config_path=examples/dev_config.json --output_dir=outputs/train/tau0 --steps 40 --log_freq 5 --save_freq 10
```
A checkpoint should be saved at step 40. The checkpoint should be saved in the directory `outputs/train/tau0/checkpoints/000040/`.

The `model.safetensors` file is not automatically generated by DeepSpeed's checkpointing during training. To consolidate the sharded model checkpoint files generated by DeepSpeed into a single `model.safetensors` file, run:
```bash
$ ./convert_checkpoint.sh outputs/train/tau0/checkpoints/000040/
```
This generates a `model.safetensors` file that can be used for inference or resuming training.

Training can be resumed by running:
```bash
$ accelerate launch lerobot/scripts/train.py --config_path=outputs/train/tau0/checkpoints/000040/train_config.json --resume=true --steps=100
```

It should be noted that the training step count continues from the checkpoint step count, but the dataloader is reset. \
Optional TODO: Restore the state of the dataloader \
https://huggingface.co/docs/accelerate/v1.6.0/en/usage_guides/checkpoint#restoring-the-state-of-the-dataloader

## Running inference with a trained model

To run inference on a trained model, you will need the saved checkpoint folder from training that contains at least `train_config.json` and `model.safetensors` files. If you ran the [checkpointing and resuming tutorial](#checkpointing-and-resuming-training-tutorial), you should be able to find the checkpoint config file at `outputs/train/tau0/checkpoints/000040/train_config.json`. Make sure you ran the `zero_to_fp32.py` and `bin_to_safetensors.py` scripts to convert the sharded model checkpoint files into a single `model.safetensors` file.

To run inference with the entire model on the same device, run:
```bash
$ python lerobot/scripts/unified_model_inference.py --config_path=outputs/train/tau0/checkpoints/000040/train_config.json
```
Running inference on the TAU0 model takes less than 8 GB of GPU memory.

For an example of how to run inference with the VLM in the cloud and the action expert on the robot, run:
```bash
$ python lerobot/scripts/cloud_robot_inference.py --config_path=outputs/train/tau0/checkpoints/000040/train_config.json
```

For Zero Shot inferencing smolvla, run:

To run inference with the entire model on the same device, run:
```bash
$ python lerobot/scripts/unified_model_inference.py --config_path=examples/train_config_smolvla.json
```

Download the smolvla weights using huggface cli
```bash
$ huggingface-cli download lerobot/smolvla_base
```


Download the SmolVLM2-500M-Video-Instruct using hugging face cli
```bash
$ huggingface-cli download HuggingFaceTB/SmolVLM2-500M-Video-Instruct
```

## Evaluating a policy in Simulation

To evaluate a policy in simulation, you can launch the `lerobot/scripts/eval.py` script with `accelerate launch`.
Each accelerate process will only work on its fraction of the tasks, improving throughput.
For example, to evaluate a policy on the LIBERO 10, run:
```bash
$ accelerate launch --config_file <ACCELERATE_CONFIG_PATH> lerobot/scripts/eval.py --config_path=outputs/train/tau0/checkpoints/000040/train_config.json
```
Note 1: You can't pass in an DeepSpeed accelerate config file to `eval.py` as DeepSpeed expects optimizer and dataloader during `accelerator.prepare()`,
which we do not provide during eval. It is recommended to pass in a DDP config.

Note 2: Make sure that the `EnvConfig` and `EvalConfig` are set to the correct values for the simulation environment in your train config file.


## Standard Data Format (for Development and Inference)

The "Standard Data Format" is the expected data format returned by `torch.utils.data.Dataset`'s `__getitem__` and the expected input to `torch.nn.Module`'s `forward` method. Any new datasets, VLMs, or VLAs that get added to this repository need to adhere to this format. Data being passed to the model during inference should also adhere to this format. The format is as follows:
```python
{
    "camera0": torch.Tensor,  # shape (C, H, W) with values from [0, 1] and with the H, W resized to the config's specifications.
    "camera1": torch.Tensor,  # shape (C, H, W) with values from [0, 1] and with the H, W resized to the config's specifications.
    # ...
    "camera{num_cams-1}": torch.Tensor,  # shape (C, H, W) with values from [0, 1] and with the H, W resized to the config's specifications.

    "local_camera0": torch.Tensor,  # shape (C, H, W) with values from [0, 1] and with the H, W resized to the config's specifications.
    "local_camera1": torch.Tensor,  # shape (C, H, W) with values from [0, 1] and with the H, W resized to the config's specifications.
    # ...
    "local_camera{action_expert_num_cams-1}": torch.Tensor,  # shape (C, H, W) with values from [0, 1] and with the H, W resized to the config's specifications.

    "state": torch.Tensor,    # shape (max_state_dim)
    "actions": torch.Tensor,  # shape (action_chunk, max_action_dim)
    "frozen_actions": torch.Tensor,  # shape (frozen_actions, max_action_dim)
    "prompt": str,            # the task prompt, e.g. "Pick up the object and place it on the table."
    "response": str,          # the response from the VLM for vision QA tasks. For LeRobotDataset, this will be an empty string.
    "loss_type": str,         # the loss type to be applied to this sample (either "CE" for cross entropy or "MSE" for mean squared error)

    "img_is_pad": torch.BoolTensor,  # shape (num_cams,) with values 0 or 1, where 1 indicates that the camera image is a padded image.
    "local_img_is_pad": torch.BoolTensor,  # shape (action_expert_num_cam,) with values 0 or 1, where 1 indicates that the local camera image is a padded image.
    "action_is_pad": torch.BoolTensor,  # shape (action_chunk,) with values 0 or 1, where 1 indicates that the action is a padded action.
    "frozen_action_is_pad": torch.BoolTensor,  # shape (frozen_actions,) with values 0 or 1, where 1 indicates that the frozen action is a padded action.
}
```
The config file will have to provide the following information in TrainPipelineConfig:
- `H, W`: The height and width of the camera images. These should be the same for all cameras.
- `num_cams`: The number of cameras for the cloud VLM in the dataset.
- `action_expert_num_cams`: The number of cameras for the action expert in the dataset.
- `max_state_dim`: The maximum dimension of the state vector.
- `max_action_dim`: The maximum dimension of the action vector.
- `action_chunk`: The number of actions in the action vector. This is usually 1 for single action tasks, but can be more for multi-action tasks.

Cameras should be labeled in order of importance (e.g. camera0 is the most important camera, camera1 is the second most important camera, etc.). The model dataset will select the most important cameras to use if num_cams is less than the number of cameras in the dataset.

Both the prompt and response strings should contain exactly one newline character at the end of the string unless they are empty strings.

## Using Simulations

### Metaworld
When using Metaworld on Ubuntu machines with headless rendering, make sure to export these environment variables:
```bash
export MUJOCO_GL=egl
export PYOPENGL_PLATFORM=egl
```


## Computing max token length for dataset mixture

Each training config (e.g., [dev-config](examples/dev_config.json)) should contain a dataset mixture definition. To evaluate the maximum token length for the dataset mixture, you can run the following command:
```bash
python lerobot/scripts/compute_max_token_length.py \
    --target_cfg=<path/to/your/training/config.json>\
    --output_path=outputs/stats/token_count.json \
    --num_workers=10
```
This will output a token count for each language key in the dataset mixture, and save it to `outputs/stats/token_count.json`. This is an example output
```json
{
 "response": {
    "1": 1.0
  },
  "prompt": {
    "51": 1.0,
    "29": 0.9813113125543276,
    "22": 0.9497392276170371,
    "16": 0.9436545386812368,
    "15": 0.9378802930584875,
    "14": 0.9096609959021482,
    "13": 0.8657022227741215,
    "12": 0.8285421582019123,
    "11": 0.8002607723829629,
    "10": 0.7618589345585496,
    "9": 0.6935303613560164,
    "8": 0.6036259778964361,
    "7": 0.5651930957407177,
    "6": 0.5206134359865888,
    "4": 0.509561654041972,
    "1": 0.5064572209114616
  }
}
```
In this dataset mixture, all `response`s have a length of 1, all `prompt`s have a length <= 51, and 98.13% of `prompt`s have a length <= 29.

## AgiBot dataset

A clone of the [`agibot-world/AgiBotWorld-Alpha`](https://huggingface.co/datasets/agibot-world/AgiBotWorld-Alpha/blob/main/scripts/convert_to_lerobot.py) dataset is provided at `/autox/teams/project-bot/AgiBotWorld-Alpha`. You can use the script `lerobot/scripts/agibot_to_lerobot.py ` to convert it to lerobot format, saved in a given directory. For example

```bash
$ python3 lerobot/scripts/agibot_to_lerobot.py --src_path /autox/teams/project-bot/AgiBotWorld-Alpha --tgt_path <path> --task_id 327
```

## Running the GitLab CI server
To run the gitlab CI server, you need to have
[docker](https://www.docker.com), [gitlab-runner](https://docs.gitlab.com/runner/),
and [Nvidia Container Toolkit](https://docs.nvidia.com/ai-enterprise/deployment/vmware/latest/docker.html) installed
on your machine. The `/autox` NAS should also be mounted for sharing checkpoints.
Once you have these installed, you can use the example config to run the gitlab CI server.
The default location for the config is `$HOME/.gitlab-runner/config.toml`.

```toml
concurrent = 4
check_interval = 0
shutdown_timeout = 0

[session_server]
  session_timeout = 1800

[[runners]]                             # Runner for (parallel) GPU jobs
  name = "sj-k8s-gpu-005"
  limit = 1                             # Only 1 GPU job at a time
  url = "https://code.autox.ds"
  id = 468
  token = "<Token generated by GitLab runner registration>"
  token_obtained_at = 2025-07-07T20:01:30Z
  token_expires_at = 0001-01-01T00:00:00Z
  executor = "docker"
  [runners.cache]
    MaxUploadedArchiveSize = 0
    [runners.cache.s3]
    [runners.cache.gcs]
    [runners.cache.azure]
  [runners.docker]
    tls_verify = false
    image = "gitlab-runner:snapshot04"  # This image needs to be built with docker/gitlab-ci-image/Dockerfile
    ipc_mode="host"                     # Ignored when shm_size is set, but good to have
    pull_policy = "if-not-present"      # Prevents pulling the image every time
    privileged = false
    disable_entrypoint_overwrite = false
    oom_kill_disable = false
    disable_cache = false
    volumes = ["/cache", "/autox:/autox:rw"]  # mounts the /autox NAS to the containers
    entrypoint = ["/bin/bash", "-c"]
    shm_size = 34359738368              # 32 GiB. Deepspeed uses more than the default 64 MiB.
    network_mtu = 0
    gpus = "\"device=0,1\""             # Use GPU 0 and 1 for the multi-GPU runner

[[runners]]                             # Runner for single-GPU jobs
  name = "sj-k8s-gpu-005"
  limit = 1
  url = "https://code.autox.ds"
  id = 471
  token = "<Token generated by GitLab runner registration>"
  token_obtained_at = 2025-07-29T20:18:22Z
  token_expires_at = 0001-01-01T00:00:00Z
  executor = "docker"
  [runners.cache]
    MaxUploadedArchiveSize = 0
    [runners.cache.s3]
    [runners.cache.gcs]
    [runners.cache.azure]
  [runners.docker]
    tls_verify = false
    image = "gitlab-runner:snapshot04"  # This image needs to be built with docker/gitlab-ci-image/Dockerfile
    pull_policy = "if-not-present"      # Prevents pulling the image every time
    privileged = false
    disable_entrypoint_overwrite = false
    oom_kill_disable = false
    gpus = "\"device=2\""               # Use GPU 2 for the single-GPU runner
    disable_cache = false
    volumes = ["/cache", "/autox:/autox:rw"]
    shm_size = 0
    network_mtu = 0

[[runners]]                             # Runner for CPU jobs
  name = "sj-k8s-gpu-005"
  limit = 2                             # Up to 2 CPU jobs at a time
  url = "https://code.autox.ds"
  id = 470
  token = "<Another token generated by GitLab runner registration>"
  token_obtained_at = 2025-07-29T18:27:09Z
  token_expires_at = 0001-01-01T00:00:00Z
  executor = "docker"
  [runners.cache]
    MaxUploadedArchiveSize = 0
    [runners.cache.s3]
    [runners.cache.gcs]
    [runners.cache.azure]
  [runners.docker]
    tls_verify = false
    image = "gitlab-runner:snapshot04"  # This image needs to be built with docker/gitlab-ci-image/Dockerfile
    pull_policy = "if-not-present"      # Prevents pulling the image every time
    privileged = false
    disable_entrypoint_overwrite = false
    oom_kill_disable = false
    disable_cache = false
    volumes = ["/cache"]
    shm_size = 0
    network_mtu = 0
```
To verify the above config works, you can manually create a container using
```bash
# Test the multi-GPU runner setup
docker container run -it --rm --gpus '"device=0,1"' --shm-size 34359738368 --ipc=host -v /autox:/autox:rw gitlab-runner:snapshot04
# Test the single-GPU runner setup
docker container run -it --rm --gpus '"device=2"' --shm-size 34359738368 --ipc=host -v /autox:/autox:rw gitlab-runner:snapshot04
# Test the CPU-only runner setup
docker container run -it --rm gitlab-runner:snapshot04
```

The gitlab runner can be started with
```bash
sudo gitlab-runner restart && gitlab-runner run
```

You can also use `gitlab-runner status` to check the status of the runner.

## Evaluating policy in a libero environment
To evaluate the policy on the LIBERO benchmark, add the following section to the training config
```json
{
    ...,
    "env": {
        "type": "libero",
        "task": "libero_spatial",
        "task_ids": [0, 2]
    },
    "eval": {
        "n_episodes": 8,
        "batch_size": 8
    },
    "eval_freq": 25,
    ...
}
```
this will run the 0th task and 2nd task in libero_spatial. Each task will run for 8 simulations in parallel.

When launched with accelerate, each GPU process will only work on its fraction of the tasks, improving throughput.

```bash

# Setting Pre-commit

Please, always set pre-commit , so it will check code styling and other necessary environment checks before committing to git.

Check if .pre-commit-config.yaml file is present under lerobot directory.
To set pre-commit use the following command

```
pre-commit install
```

After successfully executing the above command, the pre-commit checks will automatically be done whenever git commit is called. If any error is found by pre-commit, the commit to git will be made only when the error is fixed.


# Setting .ENV file

To run high level planner with gpt4o, OpenAI api key is needed to be set. Create an .env file under lerobot directory and set the variable `OPENAI_API_KEY` to your openai api key. The high level planner inference script will automatically load the api key and pass to openai client.

# Using so100_visualization script
The so100_visualization script imitates action from lerobot format dataset in simulator. To, run bi-so100-block-manipulator dataset, pull the data from git lfs and move it under lerobot/lerobot directory. Install the simulator using
```bash
$ uv sync --extra tau0 --extra pusht --extra test --extra video_benchmark --extra accelerate --extra dev --extra feetech --extra openai --extra onnx --extra smolvla --extra so100
$ source .venv/bin/activate
```


Then simply run the below script

```bash
python lerobot/scripts/so100_visualization.py --config_path=configs/so100/so100_viz_config.json
```




# Original LeRobot Repo

<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="media/lerobot-logo-thumbnail.png">
    <source media="(prefers-color-scheme: light)" srcset="media/lerobot-logo-thumbnail.png">
    <img alt="LeRobot, Hugging Face Robotics Library" src="media/lerobot-logo-thumbnail.png" style="max-width: 100%;">
  </picture>
  <br/>
  <br/>
</p>

<div align="center">

[![Tests](https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main)](https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain)
[![Coverage](https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO)](https://codecov.io/gh/huggingface/lerobot)
[![Python versions](https://img.shields.io/pypi/pyversions/lerobot)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/huggingface/lerobot/blob/main/LICENSE)
[![Status](https://img.shields.io/pypi/status/lerobot)](https://pypi.org/project/lerobot/)
[![Version](https://img.shields.io/pypi/v/lerobot)](https://pypi.org/project/lerobot/)
[![Examples](https://img.shields.io/badge/Examples-green.svg)](https://github.com/huggingface/lerobot/tree/main/examples)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg)](https://github.com/huggingface/lerobot/blob/main/CODE_OF_CONDUCT.md)
[![Discord](https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat)](https://discord.gg/s3KuuzsPFb)

</div>

<h2 align="center">
    <p><a href="https://github.com/huggingface/lerobot/blob/main/examples/10_use_so100.md">
        Build Your Own SO-100 Robot!</a></p>
</h2>

<div align="center">
  <img src="media/so100/leader_follower.webp?raw=true" alt="SO-100 leader and follower arms" title="SO-100 leader and follower arms" width="50%">

  <p><strong>Meet the SO-100 â€“ Just $110 per arm!</strong></p>
  <p>Train it in minutes with a few simple moves on your laptop.</p>
  <p>Then sit back and watch your creation act autonomously! ðŸ¤¯</p>

  <p><a href="https://github.com/huggingface/lerobot/blob/main/examples/10_use_so100.md">
      Get the full SO-100 tutorial here.</a></p>

  <p>Want to take it to the next level? Make your SO-100 mobile by building LeKiwi!</p>
  <p>Check out the <a href="https://github.com/huggingface/lerobot/blob/main/examples/11_use_lekiwi.md">LeKiwi tutorial</a> and bring your robot to life on wheels.</p>

  <img src="media/lekiwi/kiwi.webp?raw=true" alt="LeKiwi mobile robot" title="LeKiwi mobile robot" width="50%">
</div>

<br/>

<h3 align="center">
    <p>LeRobot: State-of-the-art AI for real-world robotics</p>
</h3>

---


ðŸ¤— LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.

ðŸ¤— LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.

ðŸ¤— LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.

ðŸ¤— LeRobot hosts pretrained models and datasets on this Hugging Face community page: [huggingface.co/lerobot](https://huggingface.co/lerobot)

#### Examples of pretrained models on simulation environments

<table>
  <tr>
    <td><img src="media/gym/aloha_act.gif" width="100%" alt="ACT policy on ALOHA env"/></td>
    <td><img src="media/gym/simxarm_tdmpc.gif" width="100%" alt="TDMPC policy on SimXArm env"/></td>
    <td><img src="media/gym/pusht_diffusion.gif" width="100%" alt="Diffusion policy on PushT env"/></td>
  </tr>
  <tr>
    <td align="center">ACT policy on ALOHA env</td>
    <td align="center">TDMPC policy on SimXArm env</td>
    <td align="center">Diffusion policy on PushT env</td>
  </tr>
</table>

### Acknowledgment

- Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from [ALOHA](https://tonyzhaozh.github.io/aloha) and [Mobile ALOHA](https://mobile-aloha.github.io).
- Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from [Diffusion Policy](https://diffusion-policy.cs.columbia.edu) and [UMI Gripper](https://umi-gripper.github.io).
- Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from [TDMPC](https://github.com/nicklashansen/tdmpc) and [FOWM](https://www.yunhaifeng.com/FOWM).
- Thanks to Antonio Loquercio and Ashish Kumar for their early support.
- Thanks to [Seungjae (Jay) Lee](https://sjlee.cc/), [Mahi Shafiullah](https://mahis.life/) and colleagues for open sourcing [VQ-BeT](https://sjlee.cc/vq-bet/) policy and helping us adapt the codebase to our repository. The policy is adapted from [VQ-BeT repo](https://github.com/jayLEE0301/vq_bet_official).


## Installation

Download our source code:
```bash
git clone https://github.com/huggingface/lerobot.git
cd lerobot
```

Create a virtual environment with Python 3.10 and activate it, e.g. with [`miniconda`](https://docs.anaconda.com/free/miniconda/index.html):
```bash
conda create -y -n lerobot python=3.10
conda activate lerobot
```

When using `miniconda`, install `ffmpeg` in your environment:
```bash
conda install ffmpeg -c conda-forge
```

> **NOTE:** This usually installs `ffmpeg 7.X` for your platform compiled with the `libsvtav1` encoder. If `libsvtav1` is not supported (check supported encoders with `ffmpeg -encoders`), you can:
>  - _[On any platform]_ Explicitly install `ffmpeg 7.X` using:
>  ```bash
>  conda install ffmpeg=7.1.1 -c conda-forge
>  ```
>  - _[On Linux only]_ Install [ffmpeg build dependencies](https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#GettheDependencies) and [compile ffmpeg from source with libsvtav1](https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#libsvtav1), and make sure you use the corresponding ffmpeg binary to your install with `which ffmpeg`.

Install ðŸ¤— LeRobot:
```bash
pip install -e .
```

> **NOTE:** If you encounter build errors, you may need to install additional dependencies (`cmake`, `build-essential`, and `ffmpeg libs`). On Linux, run:
`sudo apt-get install cmake build-essential python-dev pkg-config libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libswresample-dev libavfilter-dev pkg-config`. For other systems, see: [Compiling PyAV](https://pyav.org/docs/develop/overview/installation.html#bring-your-own-ffmpeg)

For simulations, ðŸ¤— LeRobot comes with gymnasium environments that can be installed as extras:
- [aloha](https://github.com/huggingface/gym-aloha)
- [xarm](https://github.com/huggingface/gym-xarm)
- [pusht](https://github.com/huggingface/gym-pusht)

For instance, to install ðŸ¤— LeRobot with aloha and pusht, use:
```bash
pip install -e ".[aloha, pusht]"
```

To use [Weights and Biases](https://docs.wandb.ai/quickstart) for experiment tracking, log in with
```bash
wandb login
```

(note: you will also need to enable WandB in the configuration. See below.)

## Walkthrough

```
.
â”œâ”€â”€ examples             # contains demonstration examples, start here to learn about LeRobot
|   â””â”€â”€ advanced         # contains even more examples for those who have mastered the basics
â”œâ”€â”€ lerobot
|   â”œâ”€â”€ configs          # contains config classes with all options that you can override in the command line
|   â”œâ”€â”€ common           # contains classes and utilities
|   |   â”œâ”€â”€ datasets       # various datasets of human demonstrations: aloha, pusht, xarm
|   |   â”œâ”€â”€ envs           # various sim environments: aloha, pusht, xarm
|   |   â”œâ”€â”€ policies       # various policies: act, diffusion, tdmpc
|   |   â”œâ”€â”€ robot_devices  # various real devices: dynamixel motors, opencv cameras, koch robots
|   |   â””â”€â”€ utils          # various utilities
|   â””â”€â”€ scripts          # contains functions to execute via command line
|       â”œâ”€â”€ eval.py                 # load policy and evaluate it on an environment
|       â”œâ”€â”€ train.py                # train a policy via imitation learning and/or reinforcement learning
|       â”œâ”€â”€ control_robot.py        # teleoperate a real robot, record data, run a policy
|       â”œâ”€â”€ push_dataset_to_hub.py  # convert your dataset into LeRobot dataset format and upload it to the Hugging Face hub
|       â””â”€â”€ visualize_dataset.py    # load a dataset and render its demonstrations
â”œâ”€â”€ outputs               # contains results of scripts execution: logs, videos, model checkpoints
â””â”€â”€ tests                 # contains pytest utilities for continuous integration
```

### Visualize datasets

Check out [example 1](./examples/1_load_lerobot_dataset.py) that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub.

You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:
```bash
python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --episode-index 0
```

or from a dataset in a local folder with the `root` option and the `--local-files-only` (in the following case the dataset will be searched for in `./my_local_data_dir/lerobot/pusht`)
```bash
python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --root ./my_local_data_dir \
    --local-files-only 1 \
    --episode-index 0
```


It will open `rerun.io` and display the camera streams, robot states and actions, like this:

https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240505T172924Z&X-Amz-Expires=300&X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&X-Amz-SignedHeaders=host&actor_id=24889239&key_id=0&repo_id=748713144


Our script can also visualize datasets stored on a distant server. See `python lerobot/scripts/visualize_dataset.py --help` for more instructions.

### The `LeRobotDataset` format

A dataset in `LeRobotDataset` format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. `dataset = LeRobotDataset("lerobot/aloha_static_coffee")` and can be indexed into like any Hugging Face and PyTorch dataset. For instance `dataset[0]` will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.

A specificity of `LeRobotDataset` is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting `delta_timestamps` to a list of relative times with respect to the indexed frame. For example, with `delta_timestamps = {"observation.image": [-1, -0.5, -0.2, 0]}`  one can retrieve, for a given index, 4 frames: 3 "previous" frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example [1_load_lerobot_dataset.py](examples/1_load_lerobot_dataset.py) for more details on `delta_timestamps`.

Under the hood, the `LeRobotDataset` format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.

Here are the important details and internal structure organization of a typical `LeRobotDataset` instantiated with `dataset = LeRobotDataset("lerobot/aloha_static_coffee")`. The exact features will change from dataset to dataset but not the main aspects:

```
dataset attributes:
  â”œ hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:
  â”‚  â”œ observation.images.cam_high (VideoFrame):
  â”‚  â”‚   VideoFrame = {'path': path to a mp4 video, 'timestamp' (float32): timestamp in the video}
  â”‚  â”œ observation.state (list of float32): position of an arm joints (for instance)
  â”‚  ... (more observations)
  â”‚  â”œ action (list of float32): goal position of an arm joints (for instance)
  â”‚  â”œ episode_index (int64): index of the episode for this sample
  â”‚  â”œ frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode
  â”‚  â”œ timestamp (float32): timestamp in the episode
  â”‚  â”œ next.done (bool): indicates the end of en episode ; True for the last frame in each episode
  â”‚  â”” index (int64): general index in the whole dataset
  â”œ episode_data_index: contains 2 tensors with the start and end indices of each episode
  â”‚  â”œ from (1D int64 tensor): first frame index for each episode â€” shape (num episodes,) starts with 0
  â”‚  â”” to: (1D int64 tensor): last frame index for each episode â€” shape (num episodes,)
  â”œ stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance
  â”‚  â”œ observation.images.cam_high: {'max': tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}
  â”‚  ...
  â”œ info: a dictionary of metadata on the dataset
  â”‚  â”œ codebase_version (str): this is to keep track of the codebase version the dataset was created with
  â”‚  â”œ fps (float): frame per second the dataset is recorded/synchronized to
  â”‚  â”œ video (bool): indicates if frames are encoded in mp4 video files to save space or stored as png files
  â”‚  â”” encoding (dict): if video, this documents the main options that were used with ffmpeg to encode the videos
  â”œ videos_dir (Path): where the mp4 videos or png images are stored/accessed
  â”” camera_keys (list of string): the keys to access camera features in the item returned by the dataset (e.g. `["observation.images.cam_high", ...]`)
```

A `LeRobotDataset` is serialised using several widespread file formats for each of its parts, namely:
- hf_dataset stored using Hugging Face datasets library serialization to parquet
- videos are stored in mp4 format to save space
- metadata are stored in plain json/jsonl files

Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the `root` argument if it's not in the default `~/.cache/huggingface/lerobot` location.

### Evaluate a pretrained policy

Check out [example 2](./examples/2_evaluate_pretrained_policy.py) that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment.

We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on [lerobot/diffusion_pusht](https://huggingface.co/lerobot/diffusion_pusht):
```bash
python lerobot/scripts/eval.py \
    --policy.path=lerobot/diffusion_pusht \
    --env.type=pusht \
    --eval.batch_size=10 \
    --eval.n_episodes=10 \
    --policy.use_amp=false \
    --policy.device=cuda
```

Note: After training your own policy, you can re-evaluate the checkpoints with:

```bash
python lerobot/scripts/eval.py --policy.path={OUTPUT_DIR}/checkpoints/last/pretrained_model
```

See `python lerobot/scripts/eval.py --help` for more instructions.

### Train your own policy

Check out [example 3](./examples/3_train_policy.py) that illustrate how to train a model using our core library in python, and [example 4](./examples/4_train_policy_with_script.md) that shows how to use our training script from command line.

To use wandb for logging training and evaluation curves, make sure you've run `wandb login` as a one-time setup step. Then, when running the training command above, enable WandB in the configuration by adding `--wandb.enable=true`.

A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of what they look like in your browser. Please also check [here](./examples/4_train_policy_with_script.md#typical-logs-and-metrics) for the explanation of some commonly used metrics in logs.

![](media/wandb.png)

Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. You may use `--eval.n_episodes=500` to evaluate on more episodes than the default. Or, after training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See `python lerobot/scripts/eval.py --help` for more instructions.

#### Reproduce state-of-the-art (SOTA)

We provide some pretrained policies on our [hub page](https://huggingface.co/lerobot) that can achieve state-of-the-art performances.
You can reproduce their training by loading the config from their run. Simply running:
```bash
python lerobot/scripts/train.py --config_path=lerobot/diffusion_pusht
```
reproduces SOTA results for Diffusion Policy on the PushT task.

## Contribute

If you would like to contribute to ðŸ¤— LeRobot, please check out our [contribution guide](https://github.com/huggingface/lerobot/blob/main/CONTRIBUTING.md).

<!-- ### Add a new dataset

To add a dataset to the hub, you need to login using a write-access token, which can be generated from the [Hugging Face settings](https://huggingface.co/settings/tokens):
```bash
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
```

Then point to your raw dataset folder (e.g. `data/aloha_static_pingpong_test_raw`), and push your dataset to the hub with:
```bash
python lerobot/scripts/push_dataset_to_hub.py \
--raw-dir data/aloha_static_pingpong_test_raw \
--out-dir data \
--repo-id lerobot/aloha_static_pingpong_test \
--raw-format aloha_hdf5
```

See `python lerobot/scripts/push_dataset_to_hub.py --help` for more instructions.

If your dataset format is not supported, implement your own in `lerobot/common/datasets/push_dataset_to_hub/${raw_format}_format.py` by copying examples like [pusht_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/pusht_zarr_format.py), [umi_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/umi_zarr_format.py), [aloha_hdf5](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/aloha_hdf5_format.py), or [xarm_pkl](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/xarm_pkl_format.py). -->


### Add a pretrained policy

Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like `${hf_user}/${repo_name}` (e.g. [lerobot/diffusion_pusht](https://huggingface.co/lerobot/diffusion_pusht)).

You first need to find the checkpoint folder located inside your experiment directory (e.g. `outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500`). Within that there is a `pretrained_model` directory which should contain:
- `config.json`: A serialized version of the policy configuration (following the policy's dataclass config).
- `model.safetensors`: A set of `torch.nn.Module` parameters, saved in [Hugging Face Safetensors](https://huggingface.co/docs/safetensors/index) format.
- `train_config.json`: A consolidated configuration containing all parameter userd for training. The policy configuration should match `config.json` exactly. Thisis useful for anyone who wants to evaluate your policy or for reproducibility.

To upload these to the hub, run the following:
```bash
huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model
```

See [eval.py](https://github.com/huggingface/lerobot/blob/main/lerobot/scripts/eval.py) for an example of how other people may use your policy.


### Improve your code with profiling

An example of a code snippet to profile the evaluation of a policy:
```python
from torch.profiler import profile, record_function, ProfilerActivity

def trace_handler(prof):
    prof.export_chrome_trace(f"tmp/trace_schedule_{prof.step_num}.json")

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    schedule=torch.profiler.schedule(
        wait=2,
        warmup=2,
        active=3,
    ),
    on_trace_ready=trace_handler
) as prof:
    with record_function("eval_policy"):
        for i in range(num_episodes):
            prof.step()
            # insert code to profile, potentially whole body of eval_policy function
```

## Citation

If you want, you can cite this work with:
```bibtex
@misc{cadene2024lerobot,
    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Wolf, Thomas},
    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},
    howpublished = "\url{https://github.com/huggingface/lerobot}",
    year = {2024}
}
```

Additionally, if you are using any of the particular policy architecture, pretrained models, or datasets, it is recommended to cite the original authors of the work as they appear below:

- [Diffusion Policy](https://diffusion-policy.cs.columbia.edu)
```bibtex
@article{chi2024diffusionpolicy,
	author = {Cheng Chi and Zhenjia Xu and Siyuan Feng and Eric Cousineau and Yilun Du and Benjamin Burchfiel and Russ Tedrake and Shuran Song},
	title ={Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},
	journal = {The International Journal of Robotics Research},
	year = {2024},
}
```
- [ACT or ALOHA](https://tonyzhaozh.github.io/aloha)
```bibtex
@article{zhao2023learning,
  title={Learning fine-grained bimanual manipulation with low-cost hardware},
  author={Zhao, Tony Z and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2304.13705},
  year={2023}
}
```

- [TDMPC](https://www.nicklashansen.com/td-mpc/)

```bibtex
@inproceedings{Hansen2022tdmpc,
	title={Temporal Difference Learning for Model Predictive Control},
	author={Nicklas Hansen and Xiaolong Wang and Hao Su},
	booktitle={ICML},
	year={2022}
}
```

- [VQ-BeT](https://sjlee.cc/vq-bet/)
```bibtex
@article{lee2024behavior,
  title={Behavior generation with latent actions},
  author={Lee, Seungjae and Wang, Yibin and Etukuru, Haritheja and Kim, H Jin and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},
  journal={arXiv preprint arXiv:2403.03181},
  year={2024}
}
```
## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=huggingface/lerobot&type=Timeline)](https://star-history.com/#huggingface/lerobot&Timeline)
