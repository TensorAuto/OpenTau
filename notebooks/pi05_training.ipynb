{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796f5526",
   "metadata": {},
   "source": [
    "* git pull\n",
    "* env installation\n",
    "* run unified_model_inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88bfbd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv -o\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a3439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5e2f340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "The token `work-oracle-node` has been saved to /home/ashah/.cache/huggingface/stored_tokens\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/ashah/.cache/huggingface/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "!hf auth login --token $HF_TOKEN --add-to-git-credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c867d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9d2fb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ashah/Documents/OpenTau/src/opentau/datasets/lerobot_dataset.py:1102: UserWarning: Using local=True with _sync=True when decorating LeRobotDataset.push_to_hub forces a wait_for_everyone. But the broadcast is not necessary returning the correct result on all nodes.\n",
      "  def push_to_hub(\n",
      "/home/ashah/Documents/OpenTau/src/opentau/datasets/lerobot_dataset.py:1159: UserWarning: Using local=True with _sync=True when decorating LeRobotDataset.pull_from_repo forces a wait_for_everyone. But the broadcast is not necessary returning the correct result on all nodes.\n",
      "  def pull_from_repo(\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ashah/Documents/OpenTau/notebooks/../src/opentau/scripts/train.py\", line 379, in <module>\n",
      "    train()\n",
      "  File \"/home/ashah/Documents/OpenTau/src/opentau/configs/parser.py\", line 388, in wrapper_inner\n",
      "    response = fn(cfg, *args, **kwargs)\n",
      "  File \"/home/ashah/Documents/OpenTau/notebooks/../src/opentau/scripts/train.py\", line 120, in train\n",
      "    cfg.validate()\n",
      "  File \"/home/ashah/Documents/OpenTau/src/opentau/configs/train.py\", line 244, in validate\n",
      "    raise FileExistsError(\n",
      "FileExistsError: Output directory ../outputs/train/pi05 already exists and resume is False. Please change your output directory so that ../outputs/train/pi05 is not overwritten.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ashah/Documents/OpenTau/.venv/bin/accelerate\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ashah/Documents/OpenTau/.venv/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
      "    args.func(args)\n",
      "  File \"/home/ashah/Documents/OpenTau/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1281, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/home/ashah/Documents/OpenTau/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 869, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/home/ashah/Documents/OpenTau/.venv/bin/python', '../src/opentau/scripts/train.py', '--config_path=../configs/examples/pi05_training_config.json', '--output_dir=../outputs/train/pi05', '--steps', '40']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "! accelerate launch ../src/opentau/scripts/train.py --config_path=../configs/examples/pi05_training_config.json --output_dir=../outputs/train/pi05 --steps 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13459ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ashah/Documents/OpenTau/src/opentau/datasets/lerobot_dataset.py:1102: UserWarning: Using local=True with _sync=True when decorating LeRobotDataset.push_to_hub forces a wait_for_everyone. But the broadcast is not necessary returning the correct result on all nodes.\n",
      "  def push_to_hub(\n",
      "/home/ashah/Documents/OpenTau/src/opentau/datasets/lerobot_dataset.py:1159: UserWarning: Using local=True with _sync=True when decorating LeRobotDataset.pull_from_repo forces a wait_for_everyone. But the broadcast is not necessary returning the correct result on all nodes.\n",
      "  def pull_from_repo(\n",
      "WARNING: cfg.job_name is deprecated and ignored. Set cfg.wandb.project and/or cfg.wandb.name instead.\n",
      "INFO 2026-01-06 10:32:03 ts/train.py:137 {'action_chunk': 10,\n",
      " 'batch_size': 2,\n",
      " 'dataloader_batch_size': 2,\n",
      " 'dataset_mixture': {'action_freq': 10.0,\n",
      "                     'datasets': [{'data_features_name_mapping': None,\n",
      "                                   'episodes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "                                   'grounding': None,\n",
      "                                   'image_transforms': {'enable': False,\n",
      "                                                        'max_num_transforms': 3,\n",
      "                                                        'random_order': False,\n",
      "                                                        'tfs': {'brightness': {'kwargs': {'brightness': [0.8,\n",
      "                                                                                                         1.2]},\n",
      "                                                                               'type': 'ColorJitter',\n",
      "                                                                               'weight': 1.0},\n",
      "                                                                'contrast': {'kwargs': {'contrast': [0.8,\n",
      "                                                                                                     1.2]},\n",
      "                                                                             'type': 'ColorJitter',\n",
      "                                                                             'weight': 1.0},\n",
      "                                                                'hue': {'kwargs': {'hue': [-0.05,\n",
      "                                                                                           0.05]},\n",
      "                                                                        'type': 'ColorJitter',\n",
      "                                                                        'weight': 1.0},\n",
      "                                                                'saturation': {'kwargs': {'saturation': [0.5,\n",
      "                                                                                                         1.5]},\n",
      "                                                                               'type': 'ColorJitter',\n",
      "                                                                               'weight': 1.0},\n",
      "                                                                'sharpness': {'kwargs': {'sharpness': [0.5,\n",
      "                                                                                                       1.5]},\n",
      "                                                                              'type': 'SharpnessJitter',\n",
      "                                                                              'weight': 1.0}}},\n",
      "                                   'loss_type_mapping': None,\n",
      "                                   'repo_id': 'physical-intelligence/libero',\n",
      "                                   'revision': None,\n",
      "                                   'root': None,\n",
      "                                   'stats': None,\n",
      "                                   'use_imagenet_stats': True,\n",
      "                                   'video_backend': 'torchcodec'}],\n",
      "                     'image_resample_strategy': 'nearest',\n",
      "                     'vector_resample_strategy': 'nearest',\n",
      "                     'weights': [1.0]},\n",
      " 'debug': False,\n",
      " 'env': None,\n",
      " 'eval': {'batch_size': 16,\n",
      "          'grid_size': None,\n",
      "          'max_episodes_rendered': 16,\n",
      "          'n_episodes': 16,\n",
      "          'recording_root': None,\n",
      "          'use_async_envs': True},\n",
      " 'eval_freq': 0,\n",
      " 'gradient_accumulation_steps': 1,\n",
      " 'job_name': 'pi05',\n",
      " 'last_checkpoint_only': True,\n",
      " 'log_freq': 5,\n",
      " 'loss_weighting': {'CE': 1.0, 'MSE': 1.0},\n",
      " 'max_action_dim': 32,\n",
      " 'max_state_dim': 32,\n",
      " 'num_cams': 2,\n",
      " 'num_workers': 16,\n",
      " 'optimizer': {'betas': [0.9, 0.95],\n",
      "               'eps': 1e-08,\n",
      "               'grad_clip_norm': 10.0,\n",
      "               'lr': 2.5e-05,\n",
      "               'type': 'adamw',\n",
      "               'weight_decay': 1e-10},\n",
      " 'output_dir': '../outputs/train/pi05',\n",
      " 'policy': {'action_decoder_latency_lower': 0.0,\n",
      "            'action_decoder_latency_mean': 0.0,\n",
      "            'action_decoder_latency_std': 0.0,\n",
      "            'action_decoder_latency_upper': 0.0,\n",
      "            'adapt_to_pi_aloha': False,\n",
      "            'attention_implementation': 'eager',\n",
      "            'chunk_size': 10,\n",
      "            'cloud_vlm_latency_lower': 0.0,\n",
      "            'cloud_vlm_latency_mean': 0.0,\n",
      "            'cloud_vlm_latency_std': 0.0,\n",
      "            'cloud_vlm_latency_upper': 0.0,\n",
      "            'device': None,\n",
      "            'discrete_action_max_length': 60,\n",
      "            'dropout': 0.1,\n",
      "            'empty_cameras': 0,\n",
      "            'freeze_vision_encoder': True,\n",
      "            'init_strategy': 'no_init',\n",
      "            'input_features': {'camera0': {'shape': [3, 224, 224],\n",
      "                                           'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
      "                               'camera1': {'shape': [3, 224, 224],\n",
      "                                           'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
      "                               'state': {'shape': [32],\n",
      "                                         'type': <FeatureType.STATE: 'STATE'>}},\n",
      "            'max_action_dim': 32,\n",
      "            'max_state_dim': 32,\n",
      "            'n_action_steps': 10,\n",
      "            'n_obs_steps': 1,\n",
      "            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
      "                                      'STATE': <NormalizationMode.MIN_MAX: 'MIN_MAX'>,\n",
      "                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},\n",
      "            'num_steps': 10,\n",
      "            'optimizer_betas': [0.9, 0.95],\n",
      "            'optimizer_eps': 1e-08,\n",
      "            'optimizer_lr': 2.5e-05,\n",
      "            'optimizer_weight_decay': 1e-10,\n",
      "            'output_features': {'actions': {'shape': [32],\n",
      "                                            'type': <FeatureType.ACTION: 'ACTION'>}},\n",
      "            'pretrained_path': '../outputs/train/pi05/checkpoints/000040',\n",
      "            'proj_width': 1024,\n",
      "            'resize_imgs_with_padding': [224, 224],\n",
      "            'scheduler_decay_lr': 2.5e-06,\n",
      "            'scheduler_decay_steps': 30000,\n",
      "            'scheduler_warmup_steps': 1000,\n",
      "            'tokenizer_max_length': 256,\n",
      "            'train_expert_only': True,\n",
      "            'type': 'pi05',\n",
      "            'use_amp': False,\n",
      "            'use_cache': True,\n",
      "            'use_delta_joint_actions_aloha': False},\n",
      " 'prefetch_factor': 8,\n",
      " 'resolution': [224, 224],\n",
      " 'resume': True,\n",
      " 'save_checkpoint': True,\n",
      " 'save_freq': 10,\n",
      " 'scheduler': {'decay_lr': 2.5e-06,\n",
      "               'num_decay_steps': 30000,\n",
      "               'num_warmup_steps': 1000,\n",
      "               'peak_lr': 2.5e-05,\n",
      "               'type': 'cosine_decay_with_warmup'},\n",
      " 'seed': 1000,\n",
      " 'steps': 100,\n",
      " 'trace_nans': True,\n",
      " 'use_policy_training_preset': True,\n",
      " 'wandb': {'allow_resume': True,\n",
      "           'disable_artifact': False,\n",
      "           'enable': True,\n",
      "           'entity': 'wyautox-autox',\n",
      "           'group': None,\n",
      "           'job_type': None,\n",
      "           'mode': None,\n",
      "           'name': None,\n",
      "           'notes': 'PI05 training run',\n",
      "           'project': 'pi05',\n",
      "           'run_id': 'p0snq9wm',\n",
      "           'tags': []}}\n",
      "INFO 2026-01-06 10:32:03 ts/train.py:141 {'_cpu': False,\n",
      " '_mixed_precision': 'bf16',\n",
      " 'backend': None,\n",
      " 'debug': False,\n",
      " 'deepspeed_plugins': None,\n",
      " 'device': 'cuda',\n",
      " 'device_mesh': None,\n",
      " 'distributed_type': 'NO',\n",
      " 'dynamo_plugin': {'backend': 'NO',\n",
      "                   'disable': False,\n",
      "                   'dynamic': False,\n",
      "                   'fullgraph': False,\n",
      "                   'mode': 'default',\n",
      "                   'options': None,\n",
      "                   'use_regional_compilation': False},\n",
      " 'fork_launched': False,\n",
      " 'local_process_index': 0,\n",
      " 'num_processes': 1,\n",
      " 'parallelism_config': None,\n",
      " 'process_index': 0,\n",
      " 'torch_tp_plugin': None,\n",
      " 'use_ipex': False}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mashah18\u001b[0m (\u001b[33mwyautox-autox\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ashah/Documents/OpenTau/notebooks/wandb/run-20260106_103204-p0snq9wm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33msplendid-dawn-32\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/wyautox-autox/pi05\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/wyautox-autox/pi05/runs/p0snq9wm\u001b[0m\n",
      "INFO 2026-01-06 10:32:05 ts/train.py:165 tracker initialized with wandb job id: p0snq9wm\n",
      "WARNING 2026-01-06 10:32:05 ts/train.py:174 Anomaly detection is enabled. This may significantly slow down training.\n",
      "INFO 2026-01-06 10:32:05 ts/train.py:178 Creating dataset\n",
      "WARNING 2026-01-06 10:32:05 ts/utils.py:614 \n",
      "The dataset you requested (physical-intelligence/libero) is in 2.0 format.\n",
      "While current version of LeRobot is backward-compatible with it, the version of your dataset still uses global\n",
      "stats instead of per-episode stats. Update your dataset stats to the new format using this command:\n",
      "```\n",
      "python src/opentau/datasets/v21/convert_dataset_v20_to_v21.py --repo-id=physical-intelligence/libero\n",
      "```\n",
      "\n",
      "If you encounter a problem, contact LeRobot maintainers on [Discord](https://discord.com/invite/s3KuuzsPFb)\n",
      "or open an [issue on GitHub](https://github.com/huggingface/lerobot/issues/new/choose).\n",
      "\n",
      "WARNING 2026-01-06 10:32:05 ts/utils.py:614 \n",
      "The dataset you requested (physical-intelligence/libero) is in 2.0 format.\n",
      "While current version of LeRobot is backward-compatible with it, the version of your dataset still uses global\n",
      "stats instead of per-episode stats. Update your dataset stats to the new format using this command:\n",
      "```\n",
      "python src/opentau/datasets/v21/convert_dataset_v20_to_v21.py --repo-id=physical-intelligence/libero\n",
      "```\n",
      "\n",
      "If you encounter a problem, contact LeRobot maintainers on [Discord](https://discord.com/invite/s3KuuzsPFb)\n",
      "or open an [issue on GitHub](https://github.com/huggingface/lerobot/issues/new/choose).\n",
      "\n",
      "INFO 2026-01-06 10:32:05 dataset.py:1095 Checking timestamps synchronization...\n",
      "INFO 2026-01-06 10:32:06 _mixture.py:327 Initializing WeightedDatasetMixture...\n",
      "INFO 2026-01-06 10:32:06 _mixture.py:350 Dataset information:\n",
      "INFO 2026-01-06 10:32:06 _mixture.py:352   - LeRobotDataset_0: Length=2758, Weight=1.0\n",
      "INFO 2026-01-06 10:32:06 _mixture.py:353 ------------------------------\n",
      "INFO 2026-01-06 10:32:06 _mixture.py:331 Total length of concatenated dataset: 2758\n",
      "INFO 2026-01-06 10:32:06 _mixture.py:372 Calculating per-sample weights...\n",
      "INFO 2026-01-06 10:32:06 _mixture.py:393   Weight for each sample in LeRobotDataset_0 (size 2758): 0.0003625816\n",
      "INFO 2026-01-06 10:32:06 _mixture.py:341 ------------------------------\n",
      "INFO 2026-01-06 10:32:06 ts/train.py:190 Creating policy\n",
      "Loading model from: ../outputs/train/pi05/checkpoints/000040\n",
      "‚úì Loaded state dict from model.safetensors\n",
      "WARNING 2026-01-06 10:32:59 ing_pi05.py:489 Vision embedding key might need handling: model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.embeddings.patch_embedding.bias\n",
      "WARNING 2026-01-06 10:32:59 ing_pi05.py:489 Vision embedding key might need handling: model.paligemma_with_expert.paligemma.model.vision_tower.vision_model.embeddings.patch_embedding.weight\n",
      "Missing keys when loading state dict: 1 keys\n",
      "  - model.paligemma_with_expert.paligemma.lm_head.weight\n",
      "INFO 2026-01-06 10:33:00 ts/train.py:193 Creating optimizer and scheduler\n",
      "INFO 2026-01-06 10:33:00 ts/train.py:201 \u001b[1m\u001b[33mOutput dir:\u001b[0m ../outputs/train/pi05\n",
      "INFO 2026-01-06 10:33:00 ts/train.py:202 cfg.steps=100 (100)\n",
      "INFO 2026-01-06 10:33:00 ts/train.py:203 num_learnable_params=701812768 (702M)\n",
      "INFO 2026-01-06 10:33:00 ts/train.py:204 num_total_params=3625148432 (4B)\n",
      "INFO 2026-01-06 10:33:00 _mixture.py:437 \n",
      "Creating DataLoader...\n",
      "INFO 2026-01-06 10:33:00 _mixture.py:438   Batch size: 2\n",
      "INFO 2026-01-06 10:33:00 _mixture.py:439   Samples per epoch (num_samples for sampler): 2758\n",
      "/home/ashah/Documents/OpenTau/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "INFO 2026-01-06 10:33:00 _mixture.py:458 DataLoader created successfully.\n",
      "INFO 2026-01-06 10:33:00 _mixture.py:459 ------------------------------\n",
      "INFO 2026-01-06 10:33:02 lerator.py:3796 Loading states from ../outputs/train/pi05/checkpoints/000040\n",
      "INFO 2026-01-06 10:33:03 pointing.py:244 All model weights loaded successfully\n",
      "INFO 2026-01-06 10:33:03 pointing.py:252 All optimizer states loaded successfully\n",
      "INFO 2026-01-06 10:33:03 pointing.py:260 All scheduler states loaded successfully\n",
      "INFO 2026-01-06 10:33:03 pointing.py:278 All dataloader sampler states loaded successfully\n",
      "INFO 2026-01-06 10:33:03 pointing.py:307 All random states loaded successfully\n",
      "INFO 2026-01-06 10:33:03 lerator.py:3893 Loading in 1 custom states\n",
      "INFO 2026-01-06 10:33:03 pointing.py:330 Loading the state of AcceleratedScheduler from ../outputs/train/pi05/checkpoints/000040/custom_checkpoint_0.pkl\n",
      "INFO 2026-01-06 10:33:03 ts/train.py:222 Resuming training from checkpoint ../outputs/train/pi05/checkpoints/000040\n",
      "INFO 2026-01-06 10:33:03 ts/train.py:243 Start offline training on a fixed dataset\n",
      "INFO 2026-01-06 10:33:14 ts/train.py:273 step:45 smpl:90 total_loss:4.864 mse_loss:0.426 ce_loss:4.438 l1_loss:0.000 accuracy:0.000 lr:1.1e-06 grad_norm:15.312\n",
      "INFO 2026-01-06 10:33:19 ts/train.py:273 step:50 smpl:100 total_loss:4.844 mse_loss:0.273 ce_loss:4.572 l1_loss:0.000 accuracy:0.000 lr:1.2e-06 grad_norm:14.688\n",
      "INFO 2026-01-06 10:33:19 lerator.py:3644 Saving current state to ../outputs/train/pi05/checkpoints/000050\n",
      "WARNING 2026-01-06 10:33:19 ls/other.py:344 Removed shared tensor {'model.paligemma_with_expert.paligemma.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "INFO 2026-01-06 10:33:28 pointing.py:114 Model weights saved in ../outputs/train/pi05/checkpoints/000050/model.safetensors\n",
      "INFO 2026-01-06 10:33:29 pointing.py:121 Optimizer state saved in ../outputs/train/pi05/checkpoints/000050/optimizer.bin\n",
      "INFO 2026-01-06 10:33:29 pointing.py:128 Scheduler state saved in ../outputs/train/pi05/checkpoints/000050/scheduler.bin\n",
      "INFO 2026-01-06 10:33:29 pointing.py:145 Sampler state for dataloader 0 saved in ../outputs/train/pi05/checkpoints/000050/sampler.bin\n",
      "INFO 2026-01-06 10:33:29 pointing.py:176 Random states saved in ../outputs/train/pi05/checkpoints/000050/random_states_0.pkl\n",
      "INFO 2026-01-06 10:33:29 pointing.py:320 Saving the state of AcceleratedScheduler to ../outputs/train/pi05/checkpoints/000050/custom_checkpoint_0.pkl\n",
      "INFO 2026-01-06 10:33:29 ts/train.py:296 Checkpoint policy after step 50\n",
      "INFO 2026-01-06 10:33:29 in_utils.py:180 Starting cleanup in '/home/ashah/Documents/OpenTau/outputs/train/pi05/checkpoints'. Keeping checkpoint: '000050'\n",
      "INFO 2026-01-06 10:33:33 in_utils.py:191 Deleting old checkpoint directory: 000040\n",
      "INFO 2026-01-06 10:33:35 in_utils.py:193 Successfully deleted 000040\n",
      "INFO 2026-01-06 10:33:41 ts/train.py:273 step:55 smpl:110 total_loss:5.010 mse_loss:0.447 ce_loss:4.563 l1_loss:0.000 accuracy:0.000 lr:1.3e-06 grad_norm:17.250\n",
      "INFO 2026-01-06 10:33:47 ts/train.py:273 step:60 smpl:120 total_loss:5.000 mse_loss:0.487 ce_loss:4.514 l1_loss:0.000 accuracy:0.000 lr:1.5e-06 grad_norm:15.812\n",
      "INFO 2026-01-06 10:33:47 lerator.py:3644 Saving current state to ../outputs/train/pi05/checkpoints/000060\n",
      "WARNING 2026-01-06 10:33:47 ls/other.py:344 Removed shared tensor {'model.paligemma_with_expert.paligemma.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "INFO 2026-01-06 10:33:55 pointing.py:114 Model weights saved in ../outputs/train/pi05/checkpoints/000060/model.safetensors\n",
      "INFO 2026-01-06 10:33:56 pointing.py:121 Optimizer state saved in ../outputs/train/pi05/checkpoints/000060/optimizer.bin\n",
      "INFO 2026-01-06 10:33:56 pointing.py:128 Scheduler state saved in ../outputs/train/pi05/checkpoints/000060/scheduler.bin\n",
      "INFO 2026-01-06 10:33:56 pointing.py:145 Sampler state for dataloader 0 saved in ../outputs/train/pi05/checkpoints/000060/sampler.bin\n",
      "INFO 2026-01-06 10:33:56 pointing.py:176 Random states saved in ../outputs/train/pi05/checkpoints/000060/random_states_0.pkl\n",
      "INFO 2026-01-06 10:33:56 pointing.py:320 Saving the state of AcceleratedScheduler to ../outputs/train/pi05/checkpoints/000060/custom_checkpoint_0.pkl\n",
      "INFO 2026-01-06 10:33:56 ts/train.py:296 Checkpoint policy after step 60\n",
      "INFO 2026-01-06 10:33:56 in_utils.py:180 Starting cleanup in '/home/ashah/Documents/OpenTau/outputs/train/pi05/checkpoints'. Keeping checkpoint: '000060'\n",
      "INFO 2026-01-06 10:33:56 in_utils.py:191 Deleting old checkpoint directory: 000050\n",
      "INFO 2026-01-06 10:34:00 in_utils.py:193 Successfully deleted 000050\n",
      "INFO 2026-01-06 10:34:06 ts/train.py:273 step:65 smpl:130 total_loss:4.955 mse_loss:0.372 ce_loss:4.583 l1_loss:0.000 accuracy:0.000 lr:1.6e-06 grad_norm:16.250\n",
      "INFO 2026-01-06 10:34:11 ts/train.py:273 step:70 smpl:140 total_loss:4.926 mse_loss:0.278 ce_loss:4.648 l1_loss:0.000 accuracy:0.000 lr:1.7e-06 grad_norm:15.125\n",
      "INFO 2026-01-06 10:34:11 lerator.py:3644 Saving current state to ../outputs/train/pi05/checkpoints/000070\n",
      "WARNING 2026-01-06 10:34:11 ls/other.py:344 Removed shared tensor {'model.paligemma_with_expert.paligemma.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "INFO 2026-01-06 10:34:20 pointing.py:114 Model weights saved in ../outputs/train/pi05/checkpoints/000070/model.safetensors\n",
      "INFO 2026-01-06 10:34:22 pointing.py:121 Optimizer state saved in ../outputs/train/pi05/checkpoints/000070/optimizer.bin\n",
      "INFO 2026-01-06 10:34:22 pointing.py:128 Scheduler state saved in ../outputs/train/pi05/checkpoints/000070/scheduler.bin\n",
      "INFO 2026-01-06 10:34:22 pointing.py:145 Sampler state for dataloader 0 saved in ../outputs/train/pi05/checkpoints/000070/sampler.bin\n",
      "INFO 2026-01-06 10:34:22 pointing.py:176 Random states saved in ../outputs/train/pi05/checkpoints/000070/random_states_0.pkl\n",
      "INFO 2026-01-06 10:34:22 pointing.py:320 Saving the state of AcceleratedScheduler to ../outputs/train/pi05/checkpoints/000070/custom_checkpoint_0.pkl\n",
      "INFO 2026-01-06 10:34:22 ts/train.py:296 Checkpoint policy after step 70\n",
      "INFO 2026-01-06 10:34:22 in_utils.py:180 Starting cleanup in '/home/ashah/Documents/OpenTau/outputs/train/pi05/checkpoints'. Keeping checkpoint: '000070'\n",
      "INFO 2026-01-06 10:34:22 in_utils.py:191 Deleting old checkpoint directory: 000060\n",
      "INFO 2026-01-06 10:34:28 in_utils.py:193 Successfully deleted 000060\n",
      "INFO 2026-01-06 10:34:34 ts/train.py:273 step:75 smpl:150 total_loss:5.161 mse_loss:0.569 ce_loss:4.592 l1_loss:0.000 accuracy:0.000 lr:1.8e-06 grad_norm:16.500\n",
      "INFO 2026-01-06 10:34:39 ts/train.py:273 step:80 smpl:160 total_loss:5.048 mse_loss:0.384 ce_loss:4.664 l1_loss:0.000 accuracy:0.000 lr:2.0e-06 grad_norm:16.750\n",
      "INFO 2026-01-06 10:34:39 lerator.py:3644 Saving current state to ../outputs/train/pi05/checkpoints/000080\n",
      "WARNING 2026-01-06 10:34:39 ls/other.py:344 Removed shared tensor {'model.paligemma_with_expert.paligemma.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "INFO 2026-01-06 10:34:49 pointing.py:114 Model weights saved in ../outputs/train/pi05/checkpoints/000080/model.safetensors\n",
      "INFO 2026-01-06 10:34:50 pointing.py:121 Optimizer state saved in ../outputs/train/pi05/checkpoints/000080/optimizer.bin\n",
      "INFO 2026-01-06 10:34:50 pointing.py:128 Scheduler state saved in ../outputs/train/pi05/checkpoints/000080/scheduler.bin\n",
      "INFO 2026-01-06 10:34:50 pointing.py:145 Sampler state for dataloader 0 saved in ../outputs/train/pi05/checkpoints/000080/sampler.bin\n",
      "INFO 2026-01-06 10:34:50 pointing.py:176 Random states saved in ../outputs/train/pi05/checkpoints/000080/random_states_0.pkl\n",
      "INFO 2026-01-06 10:34:50 pointing.py:320 Saving the state of AcceleratedScheduler to ../outputs/train/pi05/checkpoints/000080/custom_checkpoint_0.pkl\n",
      "INFO 2026-01-06 10:34:50 ts/train.py:296 Checkpoint policy after step 80\n",
      "INFO 2026-01-06 10:34:50 in_utils.py:180 Starting cleanup in '/home/ashah/Documents/OpenTau/outputs/train/pi05/checkpoints'. Keeping checkpoint: '000080'\n",
      "INFO 2026-01-06 10:34:50 in_utils.py:191 Deleting old checkpoint directory: 000070\n",
      "INFO 2026-01-06 10:34:51 in_utils.py:193 Successfully deleted 000070\n",
      "INFO 2026-01-06 10:34:57 ts/train.py:273 step:85 smpl:170 total_loss:5.386 mse_loss:0.560 ce_loss:4.826 l1_loss:0.000 accuracy:0.000 lr:2.1e-06 grad_norm:15.375\n",
      "INFO 2026-01-06 10:35:03 ts/train.py:273 step:90 smpl:180 total_loss:5.070 mse_loss:0.414 ce_loss:4.656 l1_loss:0.000 accuracy:0.000 lr:2.2e-06 grad_norm:16.000\n",
      "INFO 2026-01-06 10:35:03 lerator.py:3644 Saving current state to ../outputs/train/pi05/checkpoints/000090\n",
      "WARNING 2026-01-06 10:35:03 ls/other.py:344 Removed shared tensor {'model.paligemma_with_expert.paligemma.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "INFO 2026-01-06 10:35:15 pointing.py:114 Model weights saved in ../outputs/train/pi05/checkpoints/000090/model.safetensors\n",
      "INFO 2026-01-06 10:35:16 pointing.py:121 Optimizer state saved in ../outputs/train/pi05/checkpoints/000090/optimizer.bin\n",
      "INFO 2026-01-06 10:35:16 pointing.py:128 Scheduler state saved in ../outputs/train/pi05/checkpoints/000090/scheduler.bin\n",
      "INFO 2026-01-06 10:35:16 pointing.py:145 Sampler state for dataloader 0 saved in ../outputs/train/pi05/checkpoints/000090/sampler.bin\n",
      "INFO 2026-01-06 10:35:16 pointing.py:176 Random states saved in ../outputs/train/pi05/checkpoints/000090/random_states_0.pkl\n",
      "INFO 2026-01-06 10:35:16 pointing.py:320 Saving the state of AcceleratedScheduler to ../outputs/train/pi05/checkpoints/000090/custom_checkpoint_0.pkl\n",
      "INFO 2026-01-06 10:35:16 ts/train.py:296 Checkpoint policy after step 90\n",
      "INFO 2026-01-06 10:35:16 in_utils.py:180 Starting cleanup in '/home/ashah/Documents/OpenTau/outputs/train/pi05/checkpoints'. Keeping checkpoint: '000090'\n",
      "INFO 2026-01-06 10:35:16 in_utils.py:191 Deleting old checkpoint directory: 000080\n",
      "INFO 2026-01-06 10:35:17 in_utils.py:193 Successfully deleted 000080\n",
      "INFO 2026-01-06 10:35:23 ts/train.py:273 step:95 smpl:190 total_loss:5.029 mse_loss:0.437 ce_loss:4.592 l1_loss:0.000 accuracy:0.000 lr:2.3e-06 grad_norm:16.125\n",
      "INFO 2026-01-06 10:35:29 ts/train.py:273 step:100 smpl:200 total_loss:5.006 mse_loss:0.440 ce_loss:4.566 l1_loss:0.000 accuracy:0.000 lr:2.5e-06 grad_norm:16.750\n",
      "INFO 2026-01-06 10:35:29 lerator.py:3644 Saving current state to ../outputs/train/pi05/checkpoints/000100\n",
      "WARNING 2026-01-06 10:35:29 ls/other.py:344 Removed shared tensor {'model.paligemma_with_expert.paligemma.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "INFO 2026-01-06 10:35:41 pointing.py:114 Model weights saved in ../outputs/train/pi05/checkpoints/000100/model.safetensors\n",
      "INFO 2026-01-06 10:35:45 pointing.py:121 Optimizer state saved in ../outputs/train/pi05/checkpoints/000100/optimizer.bin\n",
      "INFO 2026-01-06 10:35:45 pointing.py:128 Scheduler state saved in ../outputs/train/pi05/checkpoints/000100/scheduler.bin\n",
      "INFO 2026-01-06 10:35:45 pointing.py:145 Sampler state for dataloader 0 saved in ../outputs/train/pi05/checkpoints/000100/sampler.bin\n",
      "INFO 2026-01-06 10:35:45 pointing.py:176 Random states saved in ../outputs/train/pi05/checkpoints/000100/random_states_0.pkl\n",
      "INFO 2026-01-06 10:35:45 pointing.py:320 Saving the state of AcceleratedScheduler to ../outputs/train/pi05/checkpoints/000100/custom_checkpoint_0.pkl\n",
      "INFO 2026-01-06 10:35:45 ts/train.py:296 Checkpoint policy after step 100\n",
      "INFO 2026-01-06 10:35:45 in_utils.py:180 Starting cleanup in '/home/ashah/Documents/OpenTau/outputs/train/pi05/checkpoints'. Keeping checkpoint: '000100'\n",
      "INFO 2026-01-06 10:35:45 in_utils.py:191 Deleting old checkpoint directory: 000090\n",
      "INFO 2026-01-06 10:35:55 in_utils.py:193 Successfully deleted 000090\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading config.yaml 8.6KB/8.6KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading config.yaml 8.6KB/8.6KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading config.yaml 8.6KB/8.6KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      Accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       CE Loss ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     Grad Norm ‚ñÉ‚ñÅ‚ñà‚ñÑ‚ñÖ‚ñÇ‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       L1 Loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Learning Rate ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      MSE Loss ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñÉ‚ñÅ‚ñà‚ñÑ‚ñà‚ñÑ‚ñÖ‚ñÖ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   Num Samples ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Training Loss ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      Accuracy 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       CE Loss 4.56635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     Grad Norm 16.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       L1 Loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Learning Rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      MSE Loss 0.44009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   Num Samples 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Training Loss 5.00645\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33msplendid-dawn-32\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/wyautox-autox/pi05/runs/p0snq9wm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/wyautox-autox/pi05\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20260106_103204-p0snq9wm/logs\u001b[0m\n",
      "INFO 2026-01-06 10:35:56 ts/train.py:370 End of training\n"
     ]
    }
   ],
   "source": [
    "! accelerate launch ../src/opentau/scripts/train.py --config_path=../outputs/train/pi05/checkpoints/000040/train_config.json --output_dir=../outputs/train/pi05 --resume=true --steps 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
