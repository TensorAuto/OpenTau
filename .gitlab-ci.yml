variables:
  GIT_LFS_SKIP_SMUDGE: "1"
  GIT_CLEAN_FLAGS: "-ffdx -e .venv/"
  PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True

before_script:
  - export PATH="$HOME/.local/bin:$PATH"
  - export UV_LINK_MODE=copy
  - export MUJOCO_GL="egl" # needed for headless rendering for metaworld
  - export PYOPENGL_PLATFORM="egl" # needed for headless rendering for metaworld
  - export PRE_COMMIT_HOME=/pre-commit-cache
  - which uv
  - uv sync --extra tau0 --extra test --extra video_benchmark --extra accelerate --extra dev --extra feetech --extra openai --extra onnx --extra smolvla --extra libero --extra metaworld
  - source .venv/bin/activate
  - which python3
  - python3 --version
  - ffmpeg -version
  - hf auth login --token $HF_TOKEN
  - wandb offline
  - df -h /dev/shm

# This job runs a training script as a regression test to ensure the main training functionality is not broken.
precommit-check:
  tags:
    - cpu
  script:
    - echo "Running pre-commit check..."
    - pre-commit run --all-files
    - echo "Pre-commit check completed successfully."


# This job runs the pytest test suite.
run-cpu-pytests:
  tags:
    - cpu
  script:
    - git config --global http.sslVerify false
    - git lfs pull --include "tests/artifacts/image_transforms/single_transforms.safetensors"
    - git lfs pull --include "tests/artifacts/image_transforms/default_transforms.safetensors"
    - echo "Running cpu based pytest and generating coverage report..."
    - pytest -m "not gpu" -n auto -v --cov=lerobot/ --cov-report=xml:cpu_test/cpu_test.xml --ignore=tests/robots --ignore=tests/motors --ignore=tests/planner/test_planner.py tests/
    - echo "Pytest execution and coverage report generation completed."
  artifacts:
    paths:
      - cpu_test/
    expire_in: 1 week

run-gpu-pytests:
  tags:
    - single-gpu
  script:
  - git config --global http.sslVerify false
  - git lfs pull --include "tests/artifacts/policies/*"
  - echo "Running gpu heavy pytest ..."
  - pytest -m "gpu" -n 0 -v --cov=lerobot/ --cov-report=xml:gpu_test/gpu_test.xml tests/
  - echo "Pytest execution and coverage report generation completed."
  artifacts:
    paths:
      - gpu_test/
    expire_in: 1 week

combine_coverage:
  tags:
    - cpu
  script:
    - uv pip install defusedxml
    - python combine_coverage.py --list_of_path=["cpu_test/cpu_test.xml","gpu_test/gpu_test.xml"] --dir="lerobot/"
  coverage: '/TOTAL\s+\d+\s+\d+\s+(\d+)/'
  needs:
    - run-cpu-pytests
    - run-gpu-pytests


# This job runs a training script as a regression test to ensure the main training functionality is not broken.
# run once with model parallelism and once with data parallelism
train-regression-test-model-parallelism:
  tags:
    - double-gpus
  script:
    - accelerate launch --config_file=configs/examples/accelerate_deepspeed_config.yaml lerobot/scripts/train.py --config_path=configs/examples/ci_config.json --output_dir=outputs/train/ci_tau0/ 2>&1 | tee /tmp/train.log
    - python3 .gitlab/scripts/check_loss_drop.py --log_path=/tmp/train.log --expected_length=25 && echo "Loss drop confirmed"
    - python3 .gitlab/scripts/check_nonzero_grad_norm.py --log_path=/tmp/train.log --expected_length=25 && echo "Non-zero grad norm confirmed"
    - python3 .gitlab/scripts/check_accumulate_grad_sync.py --log_path=/tmp/train.log --expected_length=50 && echo "Accumulate grad sync confirmed"
    - python3 .gitlab/scripts/check_state_keys.py --log_path=/tmp/train.log --source=hf && echo "Checks for state keys passed"
    - python3 lerobot/scripts/zero_to_fp32.py outputs/train/ci_tau0/checkpoints/000025 outputs/train/ci_tau0/checkpoints/000025/full_state_dict --max_shard_size 1000GB
    - python3 lerobot/scripts/bin_to_safetensors.py outputs/train/ci_tau0/checkpoints/000025/full_state_dict/pytorch_model.bin --output_file outputs/train/ci_tau0/checkpoints/000025/model.safetensors
    - mkdir -p /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/model_parallelism/checkpoints/
    - echo "$CI_JOB_ID" > /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/model_parallelism/job_id.txt
    - cp /tmp/train.log /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/model_parallelism/train.log
    - cp -r outputs/train/ci_tau0/checkpoints/000025 /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/model_parallelism/checkpoints/
    - chmod -R 777 /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/

resume-training-model-parallelism:
  tags:
    - double-gpus
  needs:
    - train-regression-test-model-parallelism
  script:
    - 'echo Loading output from job $(cat /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/model_parallelism/job_id.txt)'
    - mkdir -p outputs/train/ci_tau0/
    - cp /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/model_parallelism/train.log /tmp/train.log
    - ln -s /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/model_parallelism/checkpoints outputs/train/ci_tau0/
    - accelerate launch --config_file=configs/examples/accelerate_deepspeed_config.yaml lerobot/scripts/train.py --config_path=outputs/train/ci_tau0/checkpoints/000025/train_config.json --resume=true --steps=50 2>&1 | tee /tmp/resume.log
    - python3 .gitlab/scripts/check_loss_drop.py --log_path=/tmp/train.log --expected_length=25 --resume_log_path=/tmp/resume.log --resume_expected_length=25 && echo "Loss drop confirmed"
    - python3 .gitlab/scripts/check_nonzero_grad_norm.py --log_path=/tmp/resume.log --expected_length=25 && echo "Non-zero grad norm confirmed"
    - python3 .gitlab/scripts/check_accumulate_grad_sync.py --log_path=/tmp/resume.log --expected_length=50 && echo "Accumulate grad sync confirmed"
    - python3 .gitlab/scripts/check_state_keys.py --log_path=/tmp/resume.log --source=local && echo "Checks for state keys passed"

eval-model-unified:
  tags:
    - single-gpu
  needs:
    - train-regression-test-model-parallelism
  script:
    - 'echo Loading output from job $(cat /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/model_parallelism/job_id.txt)'
    - mkdir -p outputs/train/ci_tau0/
    - ln -s /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/model_parallelism/checkpoints outputs/train/ci_tau0/
    - ls -lR outputs
    - python3 lerobot/scripts/unified_model_inference.py --config_path=outputs/train/ci_tau0/checkpoints/000025/train_config.json

eval-model-cloud-robot-onnx:
  tags:
    - single-gpu
  needs:
    - train-regression-test-model-parallelism
  script:
#    - 'echo Loading output from job $(cat /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/model_parallelism/job_id.txt)'
#    - mkdir -p outputs/train/ci_tau0/
#    - ln -s /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/model_parallelism/checkpoints outputs/train/ci_tau0/
#    - ls -lR outputs
#    - python3 lerobot/scripts/export_to_onnx.py --config_path=outputs/train/ci_tau0/checkpoints/000025/train_config.json
    - echo "Skipping cloud-robot reference evaluation with onnx for now because are not using a 2-part policy"

libero-simulation-inference:
  tags:
    - single-gpu
  needs:
    - train-regression-test-model-parallelism
  script:
    - 'echo Loading output from job $(cat /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/model_parallelism/job_id.txt)'
    - mkdir -p outputs/train/ci_tau0/
    - ln -s /autox/teams/lerobot/pipeline/$CI_PIPELINE_ID/model_parallelism/checkpoints outputs/train/ci_tau0/
    - ls -lRL outputs
#    - accelerate launch --config_file=configs/examples/accelerate_ddp_config.yaml lerobot/scripts/eval.py --config_path=outputs/train/ci_tau0/checkpoints/000025/train_config.json
    - echo "Skipping libero simulation inference for now and will re-add after implementing rollout recording"
